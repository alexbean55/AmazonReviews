{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "\n",
    "with open('/Users/alexbean/Desktop/tfidf_data.pkl', 'rb') as file:\n",
    "    tfidf = pickle.load(file)\n",
    "with open('/Users/alexbean/Desktop/X_train_preproc.pkl', 'rb') as file:\n",
    "    X_train_preproc = pickle.load(file)\n",
    "with open('/Users/alexbean/Desktop/X_val_preproc.pkl', 'rb') as file:\n",
    "    X_val_preproc = pickle.load(file)\n",
    "with open('/Users/alexbean/Desktop/X_test_preproc.pkl', 'rb') as file:\n",
    "    X_test_preproc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('./archive/train.csv', header=None)\n",
    "train_data.columns=['polarity', 'title', 'text',]\n",
    "\n",
    "val_data = pd.read_csv('./archive/val.csv', header=None)\n",
    "val_data.columns=['polarity', 'title', 'text',]\n",
    "\n",
    "test_data = pd.read_csv('./archive/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train   = train_data['text']\n",
    "y_train   = train_data['polarity']\n",
    "\n",
    "# get val data\n",
    "X_val    = val_data['text']\n",
    "y_val    = val_data['polarity']\n",
    "\n",
    "# get test data\n",
    "X_test   = test_data['text']\n",
    "y_test   = test_data['polarity']\n",
    "\n",
    "print(f\"Train Data Shape: {X_train.shape[0]:,}\")\n",
    "print(f\"Validation Data Shape: {X_val.shape[0]:,}\")\n",
    "print(f\"Test Data Shape: {X_test.shape[0]:,}\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"Number of labels = 1 in train dataset as percentage: {((y_train == 1).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 2 in train dataset as percentage: {((y_train == 2).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"Number of labels = 1 in val dataset as percentage: {((y_val == 1).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 2 in val dataset as percentage: {((y_val == 2).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"Number of labels = 1 in test dataset as percentage: {((y_test == 1).sum() / (X_test.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 2 in test dataset as percentage: {((y_test == 2).sum() / (X_test.shape[0])) * 100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "sets_combined_x = pd.concat([X_train_preproc, X_val_preproc], ignore_index=True)\n",
    "sets_combined_y = pd.concat([y_train, y_val], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents='unicode', lowercase=True, tokenizer=tokenizer_porter, stop_words='english', max_df=0.9)\n",
    "X_train_tfidf = tfidf.fit_transform(sets_combined_x)\n",
    "print(f\"\\nTF-IDF feature matrix shape: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf.transform(X_test_preproc)\n",
    "print(f\"\\nTF-IDF feature matrix shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(**{'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 100, 'C': 1, 'random_state': 42})\n",
    "model.fit(X_train_tfidf, sets_combined_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "val_accuracy_lr = accuracy_score(y_test, y_pred) # Calculate validation accuracy\n",
    "\n",
    "print(\"\\n--- Logistic Regression (Validation) ---\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_lr:.4f}\")\n",
    "print(f\"Validation F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-567fa084caa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extract feature names from the vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get coefficients from the logistic regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Assuming model is a binary logistic regression with shape (1, n_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract feature names from the vectorizer\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Get coefficients from the logistic regression model\n",
    "# Assuming model is a binary logistic regression with shape (1, n_features)\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Sort features by their coefficient values\n",
    "# Negative coefficients are associated with negative class, positive with positive class\n",
    "sorted_indices = np.argsort(coefficients)\n",
    "\n",
    "# Adjust these numbers as needed (e.g., top 20)\n",
    "top_negative_indices = sorted_indices[:20]  # 20 most negative words\n",
    "\n",
    "# Get the corresponding words and their coefficients\n",
    "top_negative_words = [(feature_names[i], coefficients[i]) for i in top_negative_indices]\n",
    "\n",
    "# Print top negative words (aspects potentially causing negative feedback)\n",
    "print(\"Top Negative Words:\")\n",
    "for word, coef in top_negative_words:\n",
    "    print(f\"{word}: {coef:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_negative_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1bcb0c972b0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_negative_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefficients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_negative_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Prepare a list to hold the contribution scores for each comment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcomment_contributions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_negative_indices' is not defined"
     ]
    }
   ],
   "source": [
    "top_negative_words = [(feature_names[i], coefficients[i]) for i in top_negative_indices]\n",
    "\n",
    "# Prepare a list to hold the contribution scores for each comment\n",
    "comment_contributions = []\n",
    "\n",
    "# Loop through the test dataset and calculate the contribution of top negative words\n",
    "for i, comment in enumerate(X_test_preproc[y_test == 1].head(5)):\n",
    "    # Get the TF-IDF scores for the current comment\n",
    "    comment_tfidf = X_test_tfidf[i]\n",
    "    \n",
    "    # Calculate the total negative contribution of the comment (by summing contributions from top negative words)\n",
    "    total_negative_contribution = 0\n",
    "    for word, coef in top_negative_words:\n",
    "        # Get the index of the word in the vectorizer's feature names\n",
    "        if word in tfidf.get_feature_names_out():\n",
    "            word_index = tfidf.get_feature_names_out().tolist().index(word)\n",
    "            tfidf_score = comment_tfidf[0, word_index]\n",
    "            \n",
    "            # Contribution of this word to the sentiment (TF-IDF * Coefficient)\n",
    "            contribution = tfidf_score * coef\n",
    "            total_negative_contribution += contribution\n",
    "    \n",
    "    # Store the total negative contribution for this comment\n",
    "    comment_contributions.append((i, total_negative_contribution))\n",
    "\n",
    "# Sort the comments by their total negative contribution in descending order\n",
    "comment_contributions.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 10 comments with the most negative contributions\n",
    "print(\"Top 10 Comments with the Most Negative Words:\")\n",
    "\n",
    "for i, _ in comment_contributions[:10]:  # Get the top 10 comments\n",
    "    print(f\"Comment: {X_test[i]}\")\n",
    "    print(f\"Negative Contribution: {comment_contributions[i][1]:.3f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_negative_comments = pd.DataFrame({\n",
    "    'comment': [X_test[i] for i, _ in comment_contributions[:10]],  # Top 10 comments\n",
    "    'negative_contribution': [comment_contributions[i][1] for i, _ in comment_contributions[:10]]\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
