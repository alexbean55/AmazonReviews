{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941237f-7e18-4e0c-9d62-c441cb75dcfa",
   "metadata": {
    "id": "LX0ia6JVtFjr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14c627-ecd2-4ded-8b53-0ada6d89669b",
   "metadata": {
    "id": "LX0ia6JVtFjr"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247bf83-091b-438e-abff-eb46cb7ad720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c882e4-12ba-43ae-ae1f-ff85cf342360",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6453db1a-a916-4e83-a2c5-778e4c5577e7",
   "metadata": {
    "id": "mD0dQabauB7z"
   },
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1ba85-6984-4ad8-9ae3-3edabec1a4e6",
   "metadata": {
    "id": "uRBDrBxYtBbk"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train.csv', header=None)\n",
    "train_data.columns=['polarity', 'title', 'text',]\n",
    "\n",
    "val_data = pd.read_csv('../data/val.csv', header=None)\n",
    "val_data.columns=['polarity', 'title', 'text',]\n",
    "\n",
    "test_data = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5099017-00da-42e4-a973-9050849a90e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all train data\n",
    "X_train   = train_data['text']\n",
    "y_train   = train_data['polarity']\n",
    "\n",
    "# get val data\n",
    "X_val    = val_data['text']\n",
    "y_val    = val_data['polarity']\n",
    "\n",
    "# get test data\n",
    "X_test   = test_data['text']\n",
    "y_test   = test_data['polarity']\n",
    "\n",
    "print(f\"Train Data Shape: {X_train.shape[0]:,}\")\n",
    "print(f\"Validation Data Shape: {X_val.shape[0]:,}\")\n",
    "print(f\"Test Data Shape: {X_test.shape[0]:,}\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"Number of labels = 1 in train dataset as percentage: {((y_train == 1).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 2 in train dataset as percentage: {((y_train == 2).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"Number of labels = 1 in val dataset as percentage: {((y_val == 1).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 2 in val dataset as percentage: {((y_val == 2).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"Number of labels = 1 in test dataset as percentage: {((y_test == 1).sum() / (X_test.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 2 in test dataset as percentage: {((y_test == 2).sum() / (X_test.shape[0])) * 100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb029b5-9f6a-4e8b-9c27-96ef22c88a50",
   "metadata": {
    "id": "h_XKPAT4grMt"
   },
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf63b7e-75b1-4560-aa67-405dd6720cac",
   "metadata": {
    "id": "0Xlccu-qCz18"
   },
   "source": [
    "## Preprocessing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4398451f-86e3-426b-828f-1469b2d19844",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5v2_Ro6ca5I",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "efd0ad1b-a203-4b8a-a8a0-36946906f7b7"
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', str(text), flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    #added substitutions\n",
    "\n",
    "    #***********added substitutions***********\n",
    "    # remove all the special characters\n",
    "    texter = re.sub(r'\\W', ' ', texter)\n",
    "    # remove all single characters\n",
    "    texter = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove single characters from the start\n",
    "    texter = re.sub(r'\\^[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove numbers\n",
    "    texter = re.sub(r'\\d+', ' ', texter)\n",
    "    # Converting to Lowercase\n",
    "    texter = texter.lower()\n",
    "    # Remove punctuation\n",
    "    texter = re.sub(r'[^\\w\\s]', ' ', texter)\n",
    "    # Remove parentheses\n",
    "    texter = re.sub(r'\\([^)]*\\)', ' ', texter)\n",
    "    # Remove single quotes\n",
    "    texter = re.sub(r'\\'', ' ', texter)\n",
    "    # Substituting multiple spaces with single space\n",
    "    texter = re.sub(r'\\s+', ' ', texter, flags=re.I)\n",
    "\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter\n",
    "\n",
    "def clean_dataset(dataset):\n",
    "    for row in range(dataset.shape[0]):\n",
    "        dataset[row,0] = clean(dataset[row,0])\n",
    "    return dataset\n",
    "\n",
    "def tokenize_lexicon(texts):\n",
    "    return_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append(nltk.word_tokenize(texts[i]))\n",
    "        return_texts[i] = nltk.pos_tag(return_texts[i])\n",
    "    return return_texts\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "def lemmatize_texts(texts):\n",
    "    return_texts = []\n",
    "    lemmer = nltk.stem.WordNetLemmatizer()\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append([])\n",
    "        for j in range(len(texts[i])):\n",
    "                return_texts[i].append(lemmer.lemmatize(texts[i][j][0], pos=get_wordnet_pos(texts[i][j][1])))\n",
    "    return return_texts\n",
    "\n",
    "def stem_texts(texts):\n",
    "    return_texts = []\n",
    "    ps = PorterStemmer()\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append([])\n",
    "        for j in range(len(texts[i])):\n",
    "                return_texts[i].append(ps.stem(texts[i][j][0]))\n",
    "    return return_texts\n",
    "\n",
    "\n",
    "def backtostring(texts):\n",
    "    return_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append(\" \".join(texts[i]))\n",
    "    return return_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6e494-ee2d-4970-a701-4cb9139e4cb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWpMOdJ2uipq",
    "outputId": "426663c2-bce6-484a-9129-a6361491ca21"
   },
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    preproc_data = data.copy()\n",
    "    preproc_data = preproc_data.str.lower()\n",
    "\n",
    "    mapping = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    preproc_data = preproc_data.str.translate(mapping)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preproc_data = preproc_data.apply(lambda text: ' '.join([word for word in str(text).split() if word.lower() not in stop_words]))\n",
    "       \n",
    "    preproc_data = preproc_data.apply(lambda text: re.sub(r'@\\w+', '', re.sub(r'http\\S+|www\\S+', '', text)))\n",
    "    return preproc_data\n",
    "\n",
    "# get the preprocessed data\n",
    "X_train_preproc   = pre_process(X_train)\n",
    "X_train_clean_preproc   = pre_process(X_train_clean)\n",
    "X_val_preproc = pre_process(X_val)\n",
    "X_test_preproc = pre_process(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da4a1b7-0252-4a31-9522-a8bcc422087f",
   "metadata": {},
   "source": [
    "# Vectorizer (Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e221ff-830f-494b-8dff-15f3616fb1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 256\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae077c9-07bc-46eb-9e1c-2fd461e0b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9c6b5-f4f1-4a65-8388-5bc932d25066",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0e8d2-1464-4425-909e-6626786d7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n",
    "X_val = tf.keras.preprocessing.sequence.pad_sequences(X_val, padding=padding_type, truncating=trunc_type, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b467fe6-ceb6-43ec-9020-7a8b3ce4321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16140601-b210-4f08-9f77-4d081da63a1a",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d52b4e5-feab-415a-9003-4a078e7f6f95",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5362cff-1c3d-42ef-9ef5-d5a53bdb9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(128, input_dim=input_dim, activation='relu'),\n",
    "tf.keras.layers.Dropout(0.25, name=\"dropout\"),\n",
    "tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cfc65c-5790-436a-990b-22ecde00e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977227ca-aa15-4ee2-9adf-0add6bf8b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=30, shuffle=True, callbacks=[early_stop], validation_data=(X_val, y_val),  # Explicit validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec67188-3cc1-4c06-9268-bdceb315cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = history.epoch\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss', size=15)\n",
    "plt.xlabel('Epochs', size=15)\n",
    "plt.ylabel('Loss', size=15)\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy', size=15)\n",
    "plt.xlabel('Epochs', size=15)\n",
    "plt.ylabel('Accuracy', size=15)\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491418d-d33d-47a9-8354-951ebab6d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "binary_predictions = []\n",
    "\n",
    "for i in pred:\n",
    "    if i >= 0.5:\n",
    "        binary_predictions.append(1)\n",
    "    else:\n",
    "        binary_predictions.append(0)\n",
    "\n",
    "print(\"---Test Set Results---\")\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, binary_predictions)))\n",
    "print(classification_report(y_test, binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b42d2b-b991-4dbd-9190-4d15df3661f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "matrix = confusion_matrix(binary_predictions, y_test, normalize='all')\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(matrix, annot=True, ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted Labels', size=15)\n",
    "ax.set_ylabel('True Labels', size=15)\n",
    "ax.set_title('Confusion Matrix', size=15)\n",
    "ax.xaxis.set_ticklabels([0,1], size=15)\n",
    "ax.yaxis.set_ticklabels([0,1], size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad0f83-038f-4aed-bd3e-2ad187059b36",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9abd76-33db-4684-a2b3-bacd27b4b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "    tf.keras.layers.Attention(),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd25360-e529-4009-83a5-3c2a31fb27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64c1cf-f193-4b08-bedd-c3abe5357064",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=30, shuffle=True, \n",
    "                    callbacks=[early_stop], validation_data=(X_val, y_val),  # Explicit validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85a4d4-078f-4ca5-8718-dd87a2b4f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = history.epoch\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss', size=15)\n",
    "plt.xlabel('Epochs', size=15)\n",
    "plt.ylabel('Loss', size=15)\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy', size=15)\n",
    "plt.xlabel('Epochs', size=15)\n",
    "plt.ylabel('Accuracy', size=15)\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b733e3-15a7-4ca4-b913-9fc85b0325c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "binary_predictions = []\n",
    "\n",
    "for i in pred:\n",
    "    if i >= 0.5:\n",
    "        binary_predictions.append(1)\n",
    "    else:\n",
    "        binary_predictions.append(0)\n",
    "\n",
    "print(\"---Test Set Results---\")\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, binary_predictions)))\n",
    "print(classification_report(y_test, binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d6133-24b3-4aa4-919a-53f945c8e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "matrix = confusion_matrix(binary_predictions, y_test, normalize='all')\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(matrix, annot=True, ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted Labels', size=15)\n",
    "ax.set_ylabel('True Labels', size=15)\n",
    "ax.set_title('Confusion Matrix', size=15)\n",
    "ax.xaxis.set_ticklabels([0,1], size=15)\n",
    "ax.yaxis.set_ticklabels([0,1], size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5bc07-61f1-4619-b685-25e031fe44a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2144e498-3255-406a-92fb-ce039aabeff1",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f036e90-1fb6-4ba8-8df1-5df91dddbb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a46c4-bea0-4a0c-9ff1-84912aae9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained transformer model and tokenizer\n",
    "transformer_model_name = 'distilbert-base-uncased' #\"bert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "transformer_model = TFAutoModel.from_pretrained(transformer_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b58ea-e821-45e1-abc5-a6f339bd0a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input parameters\n",
    "max_length = 128  # Maximum length of the input sequence\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 768  # Output size of BERT's hidden states (depends on the model)\n",
    "num_classes = 1  # Binary classification (for sentiment analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582caeb-94c4-4d4b-a12d-e408e9191f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example preprocessing for text data\n",
    "def preprocess_data(reviews, tokenizer, max_length):\n",
    "    tokenized_data = tokenizer(\n",
    "        reviews, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    return tokenized_data[\"input_ids\"], tokenized_data[\"attention_mask\"]\n",
    "\n",
    "# Tokenize your dataset\n",
    "X_train_ids, X_train_mask = preprocess_data(train_reviews, tokenizer, max_length)\n",
    "X_val_ids, X_val_mask = preprocess_data(val_reviews, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56e666-b55b-47c5-a455-976e20043b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# 1. Transformer Encoder (Pretrained)\n",
    "transformer_output = transformer_model(input_ids, attention_mask=attention_mask)\n",
    "sequence_output = transformer_output.last_hidden_state  # Shape: (batch_size, max_length, embedding_dim)\n",
    "pooled_output = transformer_output.pooler_output  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "# 2. Attention Mechanism\n",
    "attention = tf.keras.layers.Attention()([sequence_output, sequence_output])  # Self-attention\n",
    "\n",
    "# 3. Conv1D Layer for Local Feature Extraction\n",
    "conv1d = tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu')(attention)\n",
    "global_max_pool = tf.keras.layers.GlobalMaxPooling1D()(conv1d)\n",
    "\n",
    "# 4. Fully Connected Layer\n",
    "dense = tf.keras.layers.Dense(256, activation='relu')(global_max_pool)\n",
    "dropout = tf.keras.layers.Dropout(0.4)(dense)\n",
    "\n",
    "# 5. Output Layer\n",
    "output = tf.keras.layers.Dense(num_classes, activation='sigmoid')(dropout)\n",
    "\n",
    "# Compile the Model\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss='binary_crossentropy',  # For binary sentiment classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c31735-e83e-48f6-a28d-fc56f33e7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_train_ids, X_train_mask], y_train, validation_data=([X_val_ids, X_val_mask], y_val),\n",
    "                    epochs=10, batch_size=30, shuffle=True, callbacks=[early_stop], )\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663fe6f-ca81-4025-99ed-3a4a9216fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = history.epoch\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss', size=15)\n",
    "plt.xlabel('Epochs', size=15)\n",
    "plt.ylabel('Loss', size=15)\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy', size=15)\n",
    "plt.xlabel('Epochs', size=15)\n",
    "plt.ylabel('Accuracy', size=15)\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26d068-6cf1-446d-9304-c9796e4b294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "binary_predictions = []\n",
    "\n",
    "for i in pred:\n",
    "    if i >= 0.5:\n",
    "        binary_predictions.append(1)\n",
    "    else:\n",
    "        binary_predictions.append(0)\n",
    "\n",
    "print(\"---Test Set Results---\")\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, binary_predictions)))\n",
    "print(classification_report(y_test, binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566700b-e27d-436d-b7bd-ff2dd2a1d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "matrix = confusion_matrix(binary_predictions, y_test, normalize='all')\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(matrix, annot=True, ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted Labels', size=15)\n",
    "ax.set_ylabel('True Labels', size=15)\n",
    "ax.set_title('Confusion Matrix', size=15)\n",
    "ax.xaxis.set_ticklabels([0,1], size=15)\n",
    "ax.yaxis.set_ticklabels([0,1], size=15);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
