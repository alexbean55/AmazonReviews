{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941237f-7e18-4e0c-9d62-c441cb75dcfa",
   "metadata": {
    "id": "LX0ia6JVtFjr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14c627-ecd2-4ded-8b53-0ada6d89669b",
   "metadata": {
    "id": "LX0ia6JVtFjr"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247bf83-091b-438e-abff-eb46cb7ad720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c882e4-12ba-43ae-ae1f-ff85cf342360",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6453db1a-a916-4e83-a2c5-778e4c5577e7",
   "metadata": {
    "id": "mD0dQabauB7z"
   },
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073c5635-b7db-4e20-8c14-8a21bef33e90",
   "metadata": {
    "id": "uRBDrBxYtBbk"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv')#.dropna()\n",
    "val_data = pd.read_csv('./data/val.csv')#.dropna()\n",
    "test_data = pd.read_csv('./data/test.csv')#.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb029b5-9f6a-4e8b-9c27-96ef22c88a50",
   "metadata": {
    "id": "h_XKPAT4grMt"
   },
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf63b7e-75b1-4560-aa67-405dd6720cac",
   "metadata": {
    "id": "0Xlccu-qCz18"
   },
   "source": [
    "## Preprocessing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4398451f-86e3-426b-828f-1469b2d19844",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5v2_Ro6ca5I",
    "outputId": "efd0ad1b-a203-4b8a-a8a0-36946906f7b7"
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', str(text), flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    #added substitutions\n",
    "\n",
    "    #***********added substitutions***********\n",
    "    # remove all the special characters\n",
    "    texter = re.sub(r'\\W', ' ', texter)\n",
    "    # remove all single characters\n",
    "    texter = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove single characters from the start\n",
    "    texter = re.sub(r'\\^[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove numbers\n",
    "    texter = re.sub(r'\\d+', ' ', texter)\n",
    "    # Converting to Lowercase\n",
    "    texter = texter.lower()\n",
    "    # Remove punctuation\n",
    "    texter = re.sub(r'[^\\w\\s]', ' ', texter)\n",
    "    # Remove parentheses\n",
    "    texter = re.sub(r'\\([^)]*\\)', ' ', texter)\n",
    "    # Remove single quotes\n",
    "    texter = re.sub(r'\\'', ' ', texter)\n",
    "    # Substituting multiple spaces with single space\n",
    "    texter = re.sub(r'\\s+', ' ', texter, flags=re.I)\n",
    "\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter\n",
    "\n",
    "def clean_dataset(dataset):\n",
    "    for row in range(dataset.shape[0]):\n",
    "        dataset[row,0] = clean(dataset[row,0])\n",
    "    return dataset\n",
    "\n",
    "def tokenize_lexicon(texts):\n",
    "    return_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append(nltk.word_tokenize(texts[i]))\n",
    "        return_texts[i] = nltk.pos_tag(return_texts[i])\n",
    "    return return_texts\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "def lemmatize_texts(texts):\n",
    "    return_texts = []\n",
    "    lemmer = nltk.stem.WordNetLemmatizer()\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append([])\n",
    "        for j in range(len(texts[i])):\n",
    "                return_texts[i].append(lemmer.lemmatize(texts[i][j][0], pos=get_wordnet_pos(texts[i][j][1])))\n",
    "    return return_texts\n",
    "\n",
    "def stem_texts(texts):\n",
    "    return_texts = []\n",
    "    ps = PorterStemmer()\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append([])\n",
    "        for j in range(len(texts[i])):\n",
    "                return_texts[i].append(ps.stem(texts[i][j][0]))\n",
    "    return return_texts\n",
    "\n",
    "\n",
    "def backtostring(texts):\n",
    "    return_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append(\" \".join(texts[i]))\n",
    "    return return_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6e494-ee2d-4970-a701-4cb9139e4cb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWpMOdJ2uipq",
    "outputId": "426663c2-bce6-484a-9129-a6361491ca21"
   },
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    preproc_data = data.copy()\n",
    "    preproc_data = preproc_data.str.lower()\n",
    "\n",
    "    mapping = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    preproc_data = preproc_data.str.translate(mapping)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preproc_data = preproc_data.apply(lambda text: ' '.join([word for word in str(text).split() if word.lower() not in stop_words]))\n",
    "       \n",
    "    preproc_data = preproc_data.apply(lambda text: re.sub(r'@\\w+', '', re.sub(r'http\\S+|www\\S+', '', text)))\n",
    "    return preproc_data\n",
    "\n",
    "# get the preprocessed data\n",
    "X_train_preproc   = pre_process(X_train)\n",
    "X_train_clean_preproc   = pre_process(X_train_clean)\n",
    "X_val_preproc = pre_process(X_val)\n",
    "X_test_preproc = pre_process(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da4a1b7-0252-4a31-9522-a8bcc422087f",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d6735b-5f88-4e9e-a6f3-62d6cbc59c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dba433-24c0-4e93-8ddd-a9f6bdeacfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents='unicode', lowercase=True, tokenizer=tokenizer_porter, stop_words='english')\n",
    "X_train_preproc_tfidf = tfidf.fit_transform(X_train_preproc)\n",
    "print(f\"\\nTF-IDF feature matrix shape: {X_train_preproc_tfidf.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
